{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 02:59:48.894865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754621988.905898 1266954 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754621988.909292 1266954 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-08 02:59:48.922158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[2025-08-08 02:59:50,084][INFO]: Connecting jq@dev0.uwcnc.net:3306\n",
      "[2025-08-08 02:59:50,093][INFO]: Connected jq@dev0.uwcnc.net:3306\n"
     ]
    }
   ],
   "source": [
    "# read in the credentials as environment variables\n",
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "if cwd.__contains__('src'):\n",
    "    env_vars = !cat /src/.env\n",
    "elif cwd.startswith('/data/users/jqhu'):\n",
    "    env_vars = !cat /data/users/jqhu/work/tamagotchi/smartFlies/.env\n",
    "    sys.path.append('/data/users/jqhu/work/tamagotchi/smartFlies')\n",
    "else:\n",
    "    env_vars = !cat /gscratch/walkerlab/jqhu/smartFlies/tamagotchi/.env\n",
    "\n",
    "for var in env_vars:\n",
    "    key, value = var.split('=')\n",
    "    os.environ[key] = value\n",
    "\n",
    "from schemas.schema_v6 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TrainingConfig() & f'training_config_hash=\"074eef10d15f46153fe6433861b160b8\"')\n",
    "for f in schema.jobs.fetch():\n",
    "    error = f[4].split(':')[0:2]\n",
    "    if error[0] == 'ConnectionResetError':\n",
    "        print(f[1]) # print hash\n",
    "        \n",
    "# (schema.jobs & 'key_hash=\"c89f5ef984bfb3e30b3db6ff6ae68fa6\"').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'key_to_train = {}\\nfor entry in schema.jobs.fetch():\\n    key_hash = entry[1]\\n    status = entry[4]\\n    train_hash = entry[3][\\'training_config_hash\\']\\n    key_to_train[key_hash] = [train_hash, status]\\n    \\n# find in the training config table\\nfor key_hash, tpl in key_to_train.items():\\n    train_hash = tpl[0]\\n    status = tpl[1]\\n    training_config = TrainingConfig()\\n    print(f\"key_hash: {key_hash}, train_hash: {train_hash}\")\\n    training_config = (training_config & f\\'training_config_hash=\"{train_hash}\"\\')\\n    \\n    print(training_config.fetch(\\'save_dir\\'), \"seed:\", training_config.fetch(\\'seed\\'), status)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"key_to_train = {}\n",
    "for entry in schema.jobs.fetch():\n",
    "    key_hash = entry[1]\n",
    "    status = entry[4]\n",
    "    train_hash = entry[3]['training_config_hash']\n",
    "    key_to_train[key_hash] = [train_hash, status]\n",
    "    \n",
    "# find in the training config table\n",
    "for key_hash, tpl in key_to_train.items():\n",
    "    train_hash = tpl[0]\n",
    "    status = tpl[1]\n",
    "    training_config = TrainingConfig()\n",
    "    print(f\"key_hash: {key_hash}, train_hash: {train_hash}\")\n",
    "    training_config = (training_config & f'training_config_hash=\"{train_hash}\"')\n",
    "    \n",
    "    print(training_config.fetch('save_dir'), \"seed:\", training_config.fetch('seed'), status)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schema jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.jobs\n",
    "# (schema.jobs & f'status=\"{\"error\"}\"').delete()\n",
    "# (schema.jobs & f'host=\"{\"g3123\"}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_hash  = (TrainingConfig & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/loc_algo_linear_rot_slice_const_noisy_softreset_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind/\"').fetch('training_config_hash')\n",
    "to_del = []\n",
    "for j in schema.jobs.fetch():\n",
    "    print(j[3]['training_config_hash'])\n",
    "    if j[3]['training_config_hash'] not in train_hash:\n",
    "        to_del.append(j[1])\n",
    "        \n",
    "# for h in to_del:\n",
    "    # (schema.jobs & f'key_hash=\"{h}\"').delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in schema.jobs.fetch():\n",
    "    training_config_hash = item[3]['training_config_hash']\n",
    "    # get from TrainingConfig\n",
    "    config = (TrainingConfig() & f'training_config_hash=\"{training_config_hash}\"').fetch()\n",
    "    print(config['seed'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in TrainingResult.fetch():\n",
    "    print(\"seed\",item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(hash_to_del)\n",
    "# for h in hash_to_del:\n",
    "#     (schema.jobs & f'key_hash=\"{h}\"').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  which seeds are on which host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = (schema.jobs).fetch('host')\n",
    "hosts = set(hosts)\n",
    "hosts_of_interest = ['g3103', 'g3101', 'g3100', 'g3121']\n",
    "seeds_of_interest = []\n",
    "time_substr = '2025-07-07 05'\n",
    "\n",
    "hash_to_del = []\n",
    "\n",
    "for i, host in enumerate(hosts):\n",
    "    if host not in hosts_of_interest:\n",
    "        continue\n",
    "    print(f\"Host {i+1}: {host}\")\n",
    "    for job in (schema.jobs & f'host=\"{host}\"').fetch():\n",
    "\n",
    "        seeds = (TrainingConfig() & f'training_config_hash=\"{job[3][\"training_config_hash\"]}\"').fetch('seed')\n",
    "        if seeds:\n",
    "            # print(f\"  Job: {job[1]}\")\n",
    "            # print(f\"  Training Config Hash: {job[3]['training_config_hash']}\")\n",
    "            # print(f\"  Seeds: {seeds}\")\n",
    "            seeds_of_interest.extend(seeds)\n",
    "        start_time = job[-1]\n",
    "        start_time = start_time.strftime('%Y-%m-%d %H')\n",
    "        if time_substr in start_time:\n",
    "            print(f\"  Start Time: {start_time}\")\n",
    "            print(f\"  Seeds: {seeds}\")\n",
    "        else:\n",
    "            hash_to_del.append(job[1])\n",
    "            print(f\"  Start Time: {start_time} - Not in time substring {time_substr}, deleting job with key_hash {job[1]}\")\n",
    "\n",
    "len(seeds_of_interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (schema.jobs & 'status=\"error\"' & 'host LIKE \"g%\"') # find all jobs with error status on hosts starting with 'g'\n",
    "# (schema.jobs & 'status=\"error\"' & 'host LIKE \"g%\"').delete() # find all jobs with error status on hosts starting with 'g'\n",
    "# (schema.jobs & 'status=\"error\"' ).delete() # delete all jobs with error status\n",
    "# (schema.jobs & 'host=\"a0d143aaaba8\"')\n",
    "# (schema.jobs & 'host=\"78579dfbf30a\"')\n",
    "# (schema.jobs & 'status=\"error\"' & 'host=\"78579dfbf30a\"').delete() # find all jobs with error status on hosts starting with 'g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert into `TrainingResult` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in schema.jobs.fetch():\n",
    "    training_config_hash = job[3]['training_config_hash']\n",
    "    key_hash = job[1]\n",
    "    hours_elapsed = 0\n",
    "    entry = (TrainingConfig() & f'training_config_hash=\"{training_config_hash}\"').fetch()\n",
    "    seed = entry['seed'][0] if entry['seed'] else None\n",
    "    exp = os.path.basename(os.path.dirname(str(entry['save_dir'])))\n",
    "    print(f\"Job hash: {key_hash}, Seed: {seed}, Experiment name: {exp}\")\n",
    "    \n",
    "    # TrainingResult().insert1(\n",
    "    #     dict(\n",
    "    #         training_config_hash=training_config_hash,\n",
    "    #         seed=seed,\n",
    "    #         hours_elapsed=hours_elapsed,\n",
    "    #     ),\n",
    "    #     allow_direct_insert=True\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig()\n",
    "# (TrainingConfig() & 'seed=\"1477\"')\n",
    "# TrainingConfig().fetch('save_dir')\n",
    "\n",
    "# for hash in ['c97448325b629989b2d82e8f2f325c55', '45513dd8ac9d9cdbb3a34f957436f7af', 'dee781bba1737b60f0e9b494f6acfdfc']:\n",
    "#     print((TrainingConfig() & f'training_config_hash=\"{hash}\"').fetch()[['save_dir', 'seed']])\n",
    "# (TrainingConfig() & 'training_config_hash=\"d1cc1b54c140f764025f8fc169a34456\"').fetch()[['save_dir', 'seed']]\n",
    "\n",
    "# wait to delete this too loc_algo_linear_rot_onPlume_const_diff_min_noisy_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind\n",
    "# (TrainingConfig() & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/haltere_sacd_loc_algo_rotCL_oot_const_300updates_rnn64/\"')\n",
    "# set(TrainingConfig().fetch('save_dir'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# any seed not run yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_config = (TrainingConfig() & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/finetune_oob_resetPolicy_onPlume_cosine_loc_algo_sw_dist_logstep_wind_0.1_train_std_pois_mag_narrow_wind/\"').fetch()\n",
    "print(len(subset_config))\n",
    "\n",
    "finished_seeds = TrainingResult().fetch('seed')\n",
    "queued_runs = (schema.jobs).fetch()\n",
    "queued_config_hash = [i[3]['training_config_hash'] for i in queued_runs]\n",
    "queued_seeds = (TrainingConfig & [{'training_config_hash': hash_val} for hash_val in queued_config_hash]).fetch('seed')\n",
    "\n",
    "subset_seeds = [i['seed'] for i in subset_config]\n",
    "\n",
    "# find seeds that are in subset_config but not in finished_seeds or queued_seeds\n",
    "missing_seeds = set(subset_seeds) - set(finished_seeds) - set(queued_seeds)\n",
    "print(\"Missing seeds:\", missing_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter by list values\n",
    "# config_hash = []\n",
    "# configs = (TrainingConfig() & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/loc_algo_linear_rot_slice_noisy_only_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind/\"').fetch(as_dict=True)\n",
    "# for conf in configs:\n",
    "#     if conf['dataset'] == ['constantx5b5', 'poisson_noisy3x5b5', 'poisson_mag_narrow_noisy3x5b5']:\n",
    "#         print(conf['training_config_hash'])\n",
    "#         config_hash.append(conf['training_config_hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete the training config with a specific hash\n",
    "# for hash in config_hash:\n",
    "#     (TrainingConfig() & f'training_config_hash=\"{hash}\"').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingResult.populate(reserve_jobs=True)\n",
    "# 8171898c23c5a659f66124c1e971b1b4\n",
    "# 7d7cfda61322daa8d05d557dbc8fa2c7\n",
    "# 8a14ed9755f248b49c29679f7ac785fa\n",
    "# \n",
    "# TrainingResult().insert(\n",
    "    \n",
    "#     [dict(training_config_hash='8a14ed9755f248b49c29679f7ac785fa',\n",
    "#         seed=15717,\n",
    "#         hours_elapsed=0)]\n",
    "#     , allow_direct_insert=True\n",
    "# )\n",
    "\n",
    "TrainingResult()\n",
    "# TrainingResult() & 'training_config_hash=\"3f15cb3d1c75ac2c186262cef6af8baa\"'\n",
    "# TrainingResult() & 'seed=\"6409\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_config_hash = [item[0] for item in TrainingResult().fetch()]\n",
    "# training_config = [item[0] for item in TrainingConfig().fetch()]\n",
    "# to_be_done_exp = set(training_config).difference(set(training_config_hash))\n",
    "# print(len(to_be_done_exp))\n",
    "# undone = ['/plume_10057_5e0b837fa56b6a4dffff1a034adcff23.pt',\n",
    "# '/plume_11357_85f2b252e3fb3537cc8b97e7299543d1.pt',\n",
    "# '/plume_13356_edf288d268883f38671e8d8b3c6faa80.pt',\n",
    "# '/plume_19417_4ae9997221924be5c7af5b45ebc2f399.pt',\n",
    "# '/plume_23197_87f2d20ceaa4d4567390aef42c9a18b3.pt',\n",
    "# '/plume_23311_010d39e8723185121880b9cb06092819.pt',\n",
    "# '/plume_31287_f381b0d5c73c6a567dc6a53437118d1b.pt',\n",
    "# '/plume_6337_48886f495fca96cc3bd4e5fae0eff333.pt',\n",
    "# \"/plume_8898_d8e5f14b5b73bcc1d45585b57d7f6b8c.pt\"]\n",
    "\n",
    "# undone = [exp.split('_')[-1].replace('.pt', '') for exp in undone]\n",
    "# print(len(undone))\n",
    "# print([exp for exp in to_be_done_exp if exp in undone])\n",
    "# to_be_done_exp\n",
    "# for hash in to_be_done_exp:\n",
    "#     print((TrainingConfig() & f'training_config_hash=\"{hash}\"').fetch()['seed'])\n",
    "# # (TrainingConfig() & 'training_config_hash=\"061f81f6e646a49a94eb2d27c0bf41ef\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert a hyak-based example 061824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fintune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_insert = {\n",
    " 'save_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/finetune_oot_resetPolicy_onPlume_cosine_loc_algo_sw_dist_logstep_wind_0.1_train_std_pois_mag_narrow_wind/',\n",
    "#  'seed': 2894, # to be sampled when inserting\n",
    " 'outsuffix': '', # to be built by insert \n",
    " 'num_processes': 4, # nproc=8 for 4 concurrently agents. \n",
    " 'dataset': ['poisson_noisy3x5b5', 'poisson_mag_narrow_noisy3x5b5'],\n",
    " 'num_env_steps': 10000000,\n",
    " 'birthx': 0.1, # TODO: verify this is what Toha has\n",
    " 'qvar': [0.5, 0.5], # Variance of init. location; higher = more off-plume initializations. Note this is set to 0 in evalCli \n",
    " 'diff_max': [0.8, 0.8],\n",
    " 'diff_min': [0.2, 0.2],\n",
    " 'birthx_linear_tc_steps': 0, \n",
    " 'apparent_wind': True,\n",
    " 'apparent_wind_allo': False,\n",
    " 'visual_feedback': True,\n",
    " 'birthx_max': 1.0,\n",
    " 'rnn_type': 'VRNN',\n",
    " 'hidden_size': 64,\n",
    " 'env_dt': 0.04,\n",
    " 'dryrun': False,\n",
    " 'algo': 'ppo',\n",
    " 'lr': 0.0003,\n",
    " 'eps': 1e-05,\n",
    " 'alpha': 0.99,\n",
    " 'gamma': 0.99,\n",
    " 'use_gae': True,\n",
    " 'gae_lambda': 0.95,\n",
    " 'entropy_coef': 0.005,\n",
    " 'value_loss_coef': 0.5,\n",
    " 'max_grad_norm': 0.5,\n",
    " 'cuda_deterministic': False,\n",
    " 'num_steps': 2048,\n",
    " 'ppo_epoch': 10,\n",
    " 'num_mini_batch': 4,\n",
    " 'clip_param': 0.2,\n",
    " 'log_interval': 1,\n",
    " 'save_interval': 100,\n",
    " 'no_cuda': False,\n",
    " 'use_proper_time_limits': False,\n",
    " 'recurrent_policy': True,\n",
    " 'use_linear_lr_decay': True,\n",
    " 'env_name': 'plume',\n",
    " 'log_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/logs/',\n",
    " 'dynamic': False,\n",
    " 'eval_type': 'skip',\n",
    " 'eval_episodes': 20,\n",
    " 'eval_interval': None,\n",
    " 'weight_decay': 0.0001,\n",
    " 'betadist': False,\n",
    " 'stacking': 0,\n",
    " 'masking': None,\n",
    " 'stride': 1,\n",
    " 'curriculum': False,\n",
    " 'turnx': 1.0,\n",
    " 'movex': 1.0,\n",
    " 'auto_movex': False,\n",
    " 'auto_reward': False,\n",
    " 'loc_algo': 'on_plume_linear',\n",
    " 'time_algo': 'uniform',\n",
    " 'walking': False,\n",
    " 'radiusx': 1.0,\n",
    " 'diffusion_min': 1.0,\n",
    " 'diffusion_max': 1.0,\n",
    " 'r_shaping': ['step', 'cosine', 'finetune', 'reset_actor', 'reset_critic'],\n",
    " 'wind_rel': True,\n",
    " 'action_feedback': False,\n",
    " 'squash_action': True,\n",
    " 'flipping': False,\n",
    " 'odor_scaling': True,\n",
    " 'stray_max': 2.0,\n",
    " 'test_episodes': 50,\n",
    " 'viz_episodes': 10,\n",
    " 'model_fname': '',\n",
    " 'obs_noise': 0.0, # 16deg = 0.28 rad\n",
    " 'act_noise': 0.0,\n",
    " 'cuda': True,\n",
    " 'if_vec_norm': 1,\n",
    " 'if_train_actor_std': True,\n",
    " 'rotate_by': 1, # True: rotate the wind direction and plume line randomly for generalization\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in [[9712, \"9712_334ca23297234746a0282ddd6bc6e381\"],\n",
    "[25349, \"25349_3cef330900a57fc3f9a78e19aadc452b\"],\n",
    "[31727, \"31727_6f88b79eb3b03ed34ae19e9c49f92595\"],\n",
    "[27905, \"27905_ac2d85c83f6dae93444e435b3129cad2\"],\n",
    "[5562, \"5562_771171cd71fb126cc07f3b0148a5c9a4\"],\n",
    "[4215, \"4215_e08dd86a6aa8d34eb8bbdfd7169f90c4\"],\n",
    "[3532, \"3532_dc82c1828c3f948e5147c6194d791350\"],\n",
    "[3929, \"3929_2f388a0a7560b62a46440c7b78b238b2\"],\n",
    "[5959, \"5959_428498a9f07478f9b1d6e9c284b997b0\"],\n",
    "[28594, \"28594_3c8553f83409657c8954329d30ad1471\"],\n",
    "[1686, \"1686_de9ad88ffd7e6295518474334a4a1326\"],\n",
    "[6013, \"6013_9c56357ea1392dd8556eb91b2d506f94\"]]:\n",
    "    seed = item[0]\n",
    "    hash = item[1]\n",
    "    dict_to_insert['seed'] = seed\n",
    "    dict_to_insert['outsuffix'] = hash\n",
    "    TrainingConfig.insert1(dict_to_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_insert = {\n",
    " 'save_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/loc_algo_linear_rot_oob_onPlume_noisy_sw_dist_logstep_wind_0.1_train_std_pois_mag_narrow_wind/',\n",
    "#  'outsuffix': '', # to be built by insert \n",
    " 'num_processes': 4, # nproc=8 for 4 concurrently agents. \n",
    "#  'seed': 2894, # to be sampled when inserting\n",
    " 'dataset': ['poisson_noisy3x5b5', 'poisson_mag_narrow_noisy3x5b5'],\n",
    " 'num_env_steps': 10000000,\n",
    " 'birthx': 0.1, # TODO: verify this is what Toha has\n",
    " 'qvar': [0.5, 0.5], # Variance of init. location; higher = more off-plume initializations. Note this is set to 0 in evalCli \n",
    " 'diff_max': [0.8, 0.8],\n",
    " 'diff_min': [0.25, 0.25],\n",
    " 'apparent_wind': True,\n",
    " 'apparent_wind_allo': False,\n",
    " 'visual_feedback': True,\n",
    " 'birthx_linear_tc_steps': 6, \n",
    " 'birthx_max': 1.0,\n",
    " 'rnn_type': 'VRNN',\n",
    " 'hidden_size': 64,\n",
    " 'env_dt': 0.04,\n",
    " 'dryrun': False,\n",
    " 'algo': 'ppo',\n",
    " 'lr': 0.0003,\n",
    " 'eps': 1e-05,\n",
    " 'alpha': 0.99,\n",
    " 'gamma': 0.99,\n",
    " 'use_gae': True,\n",
    " 'gae_lambda': 0.95,\n",
    " 'entropy_coef': 0.005,\n",
    " 'value_loss_coef': 0.5,\n",
    " 'max_grad_norm': 0.5,\n",
    " 'cuda_deterministic': False,\n",
    " 'num_steps': 2048,\n",
    " 'ppo_epoch': 10,\n",
    " 'num_mini_batch': 4,\n",
    " 'clip_param': 0.2,\n",
    " 'log_interval': 1,\n",
    " 'save_interval': 100,\n",
    " 'no_cuda': False,\n",
    " 'use_proper_time_limits': False,\n",
    " 'recurrent_policy': True,\n",
    " 'use_linear_lr_decay': True,\n",
    " 'env_name': 'plume',\n",
    " 'log_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/logs/',\n",
    " 'dynamic': False,\n",
    " 'eval_type': 'skip',\n",
    " 'eval_episodes': 20,\n",
    " 'eval_interval': None,\n",
    " 'weight_decay': 0.0001,\n",
    " 'betadist': False,\n",
    " 'stacking': 0,\n",
    " 'masking': None,\n",
    " 'stride': 1,\n",
    " 'curriculum': False,\n",
    " 'turnx': 1.0,\n",
    " 'movex': 1.0,\n",
    " 'auto_movex': False,\n",
    " 'auto_reward': False,\n",
    " 'loc_algo': 'on_plume_linear', # slice linear gradually increases diff_min\n",
    " 'time_algo': 'uniform',\n",
    " 'walking': False,\n",
    " 'radiusx': 1.0,\n",
    " 'diffusion_min': 1.0,\n",
    " 'diffusion_max': 1.0,\n",
    " 'r_shaping': ['step', 'oob'],\n",
    " 'wind_rel': True,\n",
    " 'action_feedback': False,\n",
    " 'squash_action': True,\n",
    " 'flipping': False,\n",
    " 'odor_scaling': True,\n",
    " 'stray_max': 2.0,\n",
    " 'test_episodes': 50,\n",
    " 'viz_episodes': 10,\n",
    " 'model_fname': '',\n",
    " 'obs_noise': 0.0, # 16deg = 0.28 rad\n",
    " 'act_noise': 0.0,\n",
    " 'cuda': True,\n",
    " 'if_vec_norm': 1,\n",
    " 'if_train_actor_std': True,\n",
    " 'rotate_by': 1, # True: rotate the wind direction and plume line randomly for generalization\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    TrainingConfig.insert1(dict_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig().fetch()['outsuffix']\n",
    "len(TrainingConfig().fetch()['outsuffix'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# short segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp name haltere_sacd_loc_algo_rotCL_oot_const_300updates_rnn128 \n",
      "reward shaping ['step', 'rotate_by', 'haltere', 'saccade'] \n",
      " loc algo slice \n",
      " curriculum 6 \n",
      " learning rate 0.0003 \n",
      " num env steps 2500000 \n"
     ]
    }
   ],
   "source": [
    "# new entry\n",
    "dict_to_insert = {\n",
    " 'save_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/haltere_sacd_loc_algo_rotCL_oot_const_300updates_rnn128/',\n",
    "#  'outsuffix': '', # to be built by insert \n",
    " 'num_processes': 4, # nproc=8 for 4 concurrently agents. \n",
    "#  'seed': 2894, # to be sampled when inserting\n",
    " 'dataset': ['constantx5b5'],\n",
    "#  'dataset': ['poisson_noisy3x5b5', 'poisson_mag_narrow_noisy3x5b5'],\n",
    "#  'num_env_steps': 10000000,\n",
    " 'num_env_steps': 2500000,\n",
    " 'birthx': 0.1, # TODO: verify this is what Toha has\n",
    " 'qvar': [1.5], # Variance of init. location; higher = more off-plume initializations. Note this is set to 0 in evalCli \n",
    " 'diff_max': [0.8],\n",
    " 'diff_min': [0.5],\n",
    " 'apparent_wind': True,\n",
    " 'apparent_wind_allo': False,\n",
    " 'visual_feedback': True,\n",
    " 'birthx_linear_tc_steps': 6, \n",
    " 'birthx_max': 1.0,\n",
    " 'rnn_type': 'VRNN',\n",
    " 'hidden_size': 128,\n",
    " 'env_dt': 0.04,\n",
    " 'dryrun': False,\n",
    " 'algo': 'ppo',\n",
    " 'lr': 0.0003,\n",
    " 'eps': 1e-05,\n",
    " 'alpha': 0.99,\n",
    " 'gamma': 0.99,\n",
    " 'use_gae': True,\n",
    " 'gae_lambda': 0.95,\n",
    " 'entropy_coef': 0.005,\n",
    " 'value_loss_coef': 0.5,\n",
    " 'max_grad_norm': 0.5,\n",
    " 'cuda_deterministic': False,\n",
    " 'num_steps': 2048,\n",
    " 'ppo_epoch': 10,\n",
    " 'num_mini_batch': 4,\n",
    " 'clip_param': 0.2,\n",
    " 'log_interval': 1,\n",
    " 'save_interval': 100,\n",
    " 'no_cuda': False,\n",
    " 'use_proper_time_limits': False,\n",
    " 'recurrent_policy': True,\n",
    " 'use_linear_lr_decay': True,\n",
    " 'env_name': 'plume',\n",
    " 'log_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/logs/',\n",
    " 'dynamic': False,\n",
    " 'eval_type': 'skip',\n",
    " 'eval_episodes': 20,\n",
    " 'eval_interval': None,\n",
    " 'weight_decay': 0.0001,\n",
    " 'betadist': False,\n",
    " 'stacking': 0,\n",
    " 'masking': None,\n",
    " 'stride': 1,\n",
    " 'curriculum': False,\n",
    " 'turnx': 1.0,\n",
    " 'movex': 1.0,\n",
    " 'auto_movex': False,\n",
    " 'auto_reward': False,\n",
    " 'loc_algo': 'slice', # slice linear gradually increases diff_min # location algorithm !!!!!\n",
    " 'time_algo': 'uniform',\n",
    " 'walking': False,\n",
    " 'radiusx': 1.0,\n",
    " 'diffusion_min': 1.0,\n",
    " 'diffusion_max': 1.0,\n",
    " 'r_shaping': ['step', 'rotate_by', 'haltere', 'saccade'],\n",
    " 'wind_rel': True,\n",
    " 'action_feedback': False,\n",
    " 'squash_action': True,\n",
    " 'flipping': False,\n",
    " 'odor_scaling': True,\n",
    " 'stray_max': 2.0,\n",
    " 'test_episodes': 50,\n",
    " 'viz_episodes': 10,\n",
    " 'model_fname': '',\n",
    " 'obs_noise': 0.0, # 16deg = 0.28 rad\n",
    " 'act_noise': 0.0,\n",
    " 'cuda': True,\n",
    " 'if_vec_norm': 1,\n",
    " 'if_train_actor_std': True,\n",
    " 'rotate_by': 1, # True: rotate the wind direction and plume line randomly for generalization\n",
    " }\n",
    "\n",
    "print('exp name', dict_to_insert['save_dir'].split('/')[-2],\n",
    "      '\\nreward shaping', dict_to_insert['r_shaping'], '\\n loc algo',  \n",
    "      dict_to_insert['loc_algo'], '\\n curriculum', \n",
    "      dict_to_insert['birthx_linear_tc_steps'], \n",
    "      '\\n learning rate', dict_to_insert['lr'],\n",
    "      '\\n num env steps', dict_to_insert['num_env_steps'],\n",
    "      '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n",
      "No seed provided. Generating a new one.\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade']\n"
     ]
    }
   ],
   "source": [
    "for i in range(18):\n",
    "    TrainingConfig.insert1(dict_to_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# staged training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_exp = 'loc_algo_rotCL_oob_onPlume_const_300updates'\n",
    "base_exp = 'loc_algo_rotCL_oot_onPlume_const_300updates'\n",
    "base_exp = 'haltere_sacd_loc_algo_rotCL_oob_on_plume_const_300updates_rnn128'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N base experiments found 30 \n",
      " reward shaping {('step', 'rotate_by', 'haltere', 'saccade')} \n",
      " loc algo {'on_plume'} \n",
      " curriculum {6} \n",
      " learning rate {0.0003} \n",
      " num env steps {2500000} \n",
      " dataset {('constantx5b5',)} \n",
      "ppo_epoch {10} \n",
      " num processes {4} \n",
      " num mini batch {4}\n",
      "\n",
      "New configs\n",
      "\n",
      "r_shaping ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams'] \n",
      "dataset ['poisson_mag_narrow_noisy3x5b5'] \n",
      "num_env_steps 5000000 \n",
      "loc_algo {'on_plume'} \n",
      "unique_birthx_steps {6}\n",
      "ppo_epoch 5 \n",
      "num_processes 8 \n",
      "num_mini_batch 8\n"
     ]
    }
   ],
   "source": [
    "base_exp = 'haltere_sacd_loc_algo_rotCL_oot_on_plume_const_300updates_rnn128'\n",
    "tbl = (TrainingConfig() & f'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/{base_exp}/\"')\n",
    "tbl = (TrainingConfig() & f'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/{base_exp}/\"')\n",
    "\n",
    "# Fetch as list of dictionaries\n",
    "rows_as_dicts = tbl.fetch(as_dict=True)\n",
    "\n",
    "# Filter: keep only rows where at least one element in r_shaping contains \"staged_\"\n",
    "rows_as_dicts = [row for row in rows_as_dicts if not any(\"staged_\" in s for s in row['r_shaping'])]\n",
    "\n",
    "unique_r_shaping = set(tuple(x['r_shaping']) for x in rows_as_dicts)\n",
    "unique_loc_algo = set(x['loc_algo'] for x in rows_as_dicts)\n",
    "unique_birthx_steps = set(x['birthx_linear_tc_steps'] for x in rows_as_dicts)\n",
    "unique_lr = set(x['lr'] for x in rows_as_dicts)\n",
    "unique_num_env_steps = set(x['num_env_steps'] for x in rows_as_dicts)\n",
    "unique_dataset = set(tuple(x['dataset']) for x in rows_as_dicts)\n",
    "unique_ppo_epoch = set(x['ppo_epoch'] for x in rows_as_dicts)\n",
    "unique_num_processes = set(x['num_processes'] for x in rows_as_dicts)\n",
    "unique_num_mini_batch = set(x['num_mini_batch'] for x in rows_as_dicts)\n",
    "\n",
    "\n",
    "print('N base experiments found', len(rows_as_dicts),\n",
    "      '\\n reward shaping', unique_r_shaping, '\\n loc algo',\n",
    "      unique_loc_algo, '\\n curriculum',\n",
    "      unique_birthx_steps,\n",
    "      '\\n learning rate', unique_lr,\n",
    "      '\\n num env steps', unique_num_env_steps,\n",
    "      '\\n dataset', unique_dataset,\n",
    "      '')\n",
    "print('ppo_epoch', unique_ppo_epoch,\n",
    "      '\\n num processes', unique_num_processes,\n",
    "      '\\n num mini batch', unique_num_mini_batch)\n",
    "r_shaping = [item for tup in unique_r_shaping for item in tup]\n",
    "r_shaping.append('staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams')\n",
    "dataset = ['poisson_mag_narrow_noisy3x5b5']\n",
    "# dataset = ['poisson_noisy3x5b5']\n",
    "# dataset = ['poisson_noisy3x5b5', 'poisson_mag_narrow_noisy3x5b5']\n",
    "\n",
    "# qvar = [0.5, 0.5]\n",
    "# diff_max = [0.8, 0.8]\n",
    "# diff_min = [0.5, 0.5]\n",
    "num_env_steps = 5000000\n",
    "qvar = [0.5]\n",
    "diff_max = [0.8]\n",
    "diff_min = [0.5]\n",
    "num_processes = 8\n",
    "num_mini_batch = 8\n",
    "ppo_epoch = 5\n",
    "\n",
    "# new config\n",
    "print('\\nNew configs')\n",
    "print('\\nr_shaping', r_shaping, \n",
    "      '\\ndataset', dataset, \n",
    "      '\\nnum_env_steps', num_env_steps,\n",
    "      '\\nloc_algo', unique_loc_algo,\n",
    "      '\\nunique_birthx_steps', unique_birthx_steps,)\n",
    "print('ppo_epoch', ppo_epoch,\n",
    "      '\\nnum_processes', num_processes,\n",
    "      '\\nnum_mini_batch', num_mini_batch,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "r_shaping: ['step', 'rotate_by', 'haltere', 'saccade', 'staged_rotCL_oot_onPlume_poisMagN3_5MilTS_sampleEParams']\n",
      "Inserted 30 new entries with modified values.\n"
     ]
    }
   ],
   "source": [
    "# for each entry in rows_as_dicts, insert a new entry with modified values\n",
    "for i, row in enumerate(rows_as_dicts):\n",
    "    new_row = row.copy()\n",
    "    new_row['r_shaping'] = r_shaping\n",
    "    new_row['dataset'] = dataset\n",
    "    new_row['qvar'] = qvar\n",
    "    new_row['diff_max'] = diff_max\n",
    "    new_row['diff_min'] = diff_min\n",
    "    new_row['num_env_steps'] = num_env_steps\n",
    "    new_row['num_processes'] = num_processes\n",
    "    new_row['num_mini_batch'] = num_mini_batch\n",
    "    new_row['ppo_epoch'] = ppo_epoch\n",
    "    \n",
    "    # Insert the modified row into the TrainingConfig table\n",
    "    TrainingConfig.insert1(new_row)\n",
    "print(f\"Inserted {len(rows_as_dicts)} new entries with modified values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaping the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

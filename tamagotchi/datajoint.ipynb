{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the credentials as environment variables\n",
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "if cwd.__contains__('src'):\n",
    "    env_vars = !cat /src/.env\n",
    "elif cwd.startswith('/data/users/jqhu'):\n",
    "    env_vars = !cat /data/users/jqhu/work/tamagotchi/smartFlies/.env\n",
    "    sys.path.append('/data/users/jqhu/work/tamagotchi/smartFlies')\n",
    "else:\n",
    "    env_vars = !cat /gscratch/walkerlab/jqhu/smartFlies/tamagotchi/.env\n",
    "\n",
    "for var in env_vars:\n",
    "    key, value = var.split('=')\n",
    "    os.environ[key] = value\n",
    "\n",
    "from schemas.schema_v4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TrainingConfig() & f'training_config_hash=\"074eef10d15f46153fe6433861b160b8\"')\n",
    "for f in schema.jobs.fetch():\n",
    "    error = f[4].split(':')[0:2]\n",
    "    if error[0] == 'ConnectionResetError':\n",
    "        print(f[1]) # print hash\n",
    "        \n",
    "# (schema.jobs & 'key_hash=\"c89f5ef984bfb3e30b3db6ff6ae68fa6\"').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_train = {}\n",
    "for entry in schema.jobs.fetch():\n",
    "    key_hash = entry[1]\n",
    "    status = entry[4]\n",
    "    train_hash = entry[3]['training_config_hash']\n",
    "    key_to_train[key_hash] = [train_hash, status]\n",
    "    \n",
    "# find in the training config table\n",
    "for key_hash, tpl in key_to_train.items():\n",
    "    train_hash = tpl[0]\n",
    "    status = tpl[1]\n",
    "    training_config = TrainingConfig()\n",
    "    print(f\"key_hash: {key_hash}, train_hash: {train_hash}\")\n",
    "    training_config = (training_config & f'training_config_hash=\"{train_hash}\"')\n",
    "    \n",
    "    print(training_config.fetch('save_dir'), \"seed:\", training_config.fetch('seed'), status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.jobs\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "# print((TrainingConfig() & f'training_config_hash=\"e169334e293b3b147111af579743d43a\"').fetch()[['save_dir', 'seed']])\n",
    "# (schema.jobs & 'host=\"g3112\"').fetch() # CNC DEV MACHINE\n",
    "\n",
    "# (schema.jobs & 'key_hash=\"ecb15d226575cf3c8716c339bd1eeb30\"').delete()\n",
    "# (schema.jobs & 'status=\"error\"' & 'key_hash=\"0bb9a16627e73f4077ef6a5c2fbc67b9\"').delete()\n",
    "# (schema.jobs & 'key_hash=\"93935d32595f366ec6d0adf707e801be\"').delete()\n",
    "\n",
    "# schema.jobs.delete()\n",
    "# training_config_hash_to_delete = [\"8171898c23c5a659f66124c1e971b1b4\", \"7d7cfda61322daa8d05d557dbc8fa2c7\", \"8a14ed9755f248b49c29679f7ac785fa\"]\n",
    "# corresponding_ket_hash = [\"001e39a0d09286e887e89d0c93f3e1da\", \"5ed4bd5907d1356fa70a56fe5c1e0f91\"]\n",
    "\n",
    "\n",
    "###### find the training config hash and the seed to insert into the training result table  ########\n",
    "\n",
    "# result_to_insert = {}\n",
    "# for i in (schema.jobs & 'status=\"error\"').fetch():\n",
    "#     conf = TrainingConfig() & f'training_config_hash=\"{i[3][\"training_config_hash\"]}\"'\n",
    "#     if conf:\n",
    "#         print(i[1], i[3]['training_config_hash'], conf.fetch('seed')[0], conf.fetch('save_dir'))\n",
    "#         result_to_insert[i[1]] = [i[3]['training_config_hash'], conf.fetch('seed')[0]]\n",
    "#     else:\n",
    "#         print(i[1], i[3]['training_config_hash'], \"not found in training config\")\n",
    "\n",
    "# corresponding_ket_hash = result_to_insert.keys()\n",
    "# for h in corresponding_ket_hash:\n",
    "#     print((schema.jobs & f'key_hash=\"{h}\"'))\n",
    "#     (schema.jobs & f'key_hash=\"{h}\"').delete()\n",
    "\n",
    "# for k in result_to_insert.keys():\n",
    "#     print(k, result_to_insert[k][0], result_to_insert[k][1])\n",
    "#     TrainingResult().insert(\n",
    "#         [dict(training_config_hash=result_to_insert[k][0],\n",
    "#             seed=result_to_insert[k][1],\n",
    "#             hours_elapsed=0)]\n",
    "#         , allow_direct_insert=True\n",
    "#     )\n",
    "# TrainingResult().insert(\n",
    "    \n",
    "#     [dict(training_config_hash='e169334e293b3b147111af579743d43a',\n",
    "#         seed=22720,\n",
    "#         hours_elapsed=0)]\n",
    "#     , allow_direct_insert=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig()\n",
    "# (TrainingConfig() & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/gnODE_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind/\"' & 'rnn_type=\"GNODE\"')\n",
    "# (TrainingConfig() & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/dt0.1_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind/\"')\n",
    "# (TrainingConfig() & 'seed=\"1477\"')\n",
    "# (TrainingConfig() & 'loc_algo=\"quantile\"')\n",
    "# (TrainingConfig() & 'env_dt=\"0.1\"')\n",
    "# (TrainingConfig() & 'rnn_type=\"GNODE\"')\n",
    "\n",
    "# (TrainingConfig() & 'save_dir=\"/src/data/wind_sensing/apparent_wind_visual_feedback/dt0.1_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind\"')\n",
    "\n",
    "# TrainingConfig().fetch('save_dir')\n",
    "\n",
    "\n",
    "# for hash in ['c97448325b629989b2d82e8f2f325c55', '45513dd8ac9d9cdbb3a34f957436f7af', 'dee781bba1737b60f0e9b494f6acfdfc']:\n",
    "#     print((TrainingConfig() & f'training_config_hash=\"{hash}\"').fetch()[['save_dir', 'seed']])\n",
    "# (TrainingConfig() & 'training_config_hash=\"d1cc1b54c140f764025f8fc169a34456\"').fetch()[['save_dir', 'seed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TrainingConfig() & 'training_config_hash=\"8a14ed9755f248b49c29679f7ac785fa\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingResult.populate(reserve_jobs=True)\n",
    "# 8171898c23c5a659f66124c1e971b1b4\n",
    "# 7d7cfda61322daa8d05d557dbc8fa2c7\n",
    "# 8a14ed9755f248b49c29679f7ac785fa\n",
    "# \n",
    "# TrainingResult().insert(\n",
    "    \n",
    "#     [dict(training_config_hash='8a14ed9755f248b49c29679f7ac785fa',\n",
    "#         seed=15717,\n",
    "#         hours_elapsed=0)]\n",
    "#     , allow_direct_insert=True\n",
    "# )\n",
    "\n",
    "# TrainingResult() & 'training_config_hash=\"8a14ed9755f248b49c29679f7ac785fa\"'\n",
    "TrainingResult()\n",
    "TrainingResult() & 'seed=\"6409\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config_hash = [item[0] for item in TrainingResult().fetch()]\n",
    "training_config = [item[0] for item in TrainingConfig().fetch()]\n",
    "to_be_done_exp = set(training_config).difference(set(training_config_hash))\n",
    "print(len(to_be_done_exp))\n",
    "undone = ['/plume_10057_5e0b837fa56b6a4dffff1a034adcff23.pt',\n",
    "'/plume_11357_85f2b252e3fb3537cc8b97e7299543d1.pt',\n",
    "'/plume_13356_edf288d268883f38671e8d8b3c6faa80.pt',\n",
    "'/plume_19417_4ae9997221924be5c7af5b45ebc2f399.pt',\n",
    "'/plume_23197_87f2d20ceaa4d4567390aef42c9a18b3.pt',\n",
    "'/plume_23311_010d39e8723185121880b9cb06092819.pt',\n",
    "'/plume_31287_f381b0d5c73c6a567dc6a53437118d1b.pt',\n",
    "'/plume_6337_48886f495fca96cc3bd4e5fae0eff333.pt',\n",
    "\"/plume_8898_d8e5f14b5b73bcc1d45585b57d7f6b8c.pt\"]\n",
    "\n",
    "undone = [exp.split('_')[-1].replace('.pt', '') for exp in undone]\n",
    "print(len(undone))\n",
    "print([exp for exp in to_be_done_exp if exp in undone])\n",
    "to_be_done_exp\n",
    "for hash in to_be_done_exp:\n",
    "    print((TrainingConfig() & f'training_config_hash=\"{hash}\"').fetch()['seed'])\n",
    "# (TrainingConfig() & 'training_config_hash=\"061f81f6e646a49a94eb2d27c0bf41ef\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert a hyak-based example 061824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_insert = {\n",
    " 'save_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/gnODE_loc_algo_sw_dist_logstep_wind_0.01_train_std_pois_mag_narrow_wind/',\n",
    "#  'outsuffix': '', # to be built by insert \n",
    " 'num_processes': 4, # nproc=8 for 4 concurrently agents. \n",
    "#  'seed': 2894, # to be sampled when inserting\n",
    " 'dataset': ['constantx5b5', 'constant_mag_narrowx5b5', 'poisson_noisy3x5b5', 'poisson_mag_narrow_noisy3x5b5'],\n",
    " 'num_env_steps': 10000000,\n",
    " 'birthx': 0.01, # TODO: verify this is what Toha has\n",
    " 'qvar': [2.0, 1.0, 0.5, 0.5], # Variance of init. location; higher = more off-plume initializations. Note this is set to 0 in evalCli \n",
    " 'diff_max': [0.8, 0.8, 0.8, 0.8],\n",
    " 'diff_min': [0.7, 0.7, 0.4, 0.4],\n",
    " 'apparent_wind': True,\n",
    " 'apparent_wind_allo': False,\n",
    " 'visual_feedback': True,\n",
    " 'birthx_linear_tc_steps': 7, \n",
    " 'birthx_max': 1.0,\n",
    " 'rnn_type': 'GNODE',\n",
    " 'hidden_size': 16,\n",
    " 'env_dt': 0.04,\n",
    " 'dryrun': False,\n",
    " 'algo': 'ppo',\n",
    " 'lr': 0.0003,\n",
    " 'eps': 1e-05,\n",
    " 'alpha': 0.99,\n",
    " 'gamma': 0.99,\n",
    " 'use_gae': True,\n",
    " 'gae_lambda': 0.95,\n",
    " 'entropy_coef': 0.005,\n",
    " 'value_loss_coef': 0.5,\n",
    " 'max_grad_norm': 0.5,\n",
    " 'cuda_deterministic': False,\n",
    " 'num_steps': 2048,\n",
    " 'ppo_epoch': 10,\n",
    " 'num_mini_batch': 4,\n",
    " 'clip_param': 0.2,\n",
    " 'log_interval': 1,\n",
    " 'save_interval': 100,\n",
    " 'no_cuda': False,\n",
    " 'use_proper_time_limits': False,\n",
    " 'recurrent_policy': True,\n",
    " 'use_linear_lr_decay': True,\n",
    " 'env_name': 'plume',\n",
    " 'log_dir': '/src/data/wind_sensing/apparent_wind_visual_feedback/logs/',\n",
    " 'dynamic': False,\n",
    " 'eval_type': 'skip',\n",
    " 'eval_episodes': 20,\n",
    " 'eval_interval': None,\n",
    " 'weight_decay': 0.0001,\n",
    " 'betadist': False,\n",
    " 'stacking': 0,\n",
    " 'masking': None,\n",
    " 'stride': 1,\n",
    " 'curriculum': False,\n",
    " 'turnx': 1.0,\n",
    " 'movex': 1.0,\n",
    " 'auto_movex': False,\n",
    " 'auto_reward': False,\n",
    " 'loc_algo': 'quantile',\n",
    " 'time_algo': 'uniform',\n",
    " 'walking': False,\n",
    " 'radiusx': 1.0,\n",
    " 'diffusion_min': 1.0,\n",
    " 'diffusion_max': 1.0,\n",
    " 'r_shaping': ['step', 'oob'],\n",
    " 'wind_rel': True,\n",
    " 'action_feedback': False,\n",
    " 'squash_action': True,\n",
    " 'flipping': True,\n",
    " 'odor_scaling': True,\n",
    " 'stray_max': 2.0,\n",
    " 'test_episodes': 50,\n",
    " 'viz_episodes': 10,\n",
    " 'model_fname': '',\n",
    " 'obs_noise': 0.0, # 16deg = 0.28 rad\n",
    " 'act_noise': 0.0,\n",
    " 'cuda': True,\n",
    " 'if_vec_norm': 1,\n",
    " 'if_train_actor_std': True,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    TrainingConfig.insert1(dict_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig().fetch()['outsuffix']\n",
    "len(TrainingConfig().fetch()['outsuffix'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering table entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig & 'lr < 1'\n",
    "TrainingConfig().fetch()['save_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaping the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
